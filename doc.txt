This project generates data, loads it into a MySQL database, and then migrates that data into a PostgreSQL database.
It consists of several scripts and configuration files, including a Docker Compose file that runs the required database
containers.

File docker-compose.yml
The docker-compose.yml file is used to run two Docker containers: one for MySQL and one for PostgreSQL. This allows both
databases to run in isolated environments without the need to install local instances of these databases.
To run the containers, please execute the following line in the terminal:
docker-compose up -d

File generate_data.py
This script generates sample data in CSV file, which will later be loaded to the first container with MySQL database.
First column ('alphabets') contains random strings and the second column 'numbers' contains simply numbers.
Functions:
sample_data_generator(file_path, num_rows): wll produce 2 million rows which is a quite big data set (over 1 GB size).
This function takes two parameters file_path - specify the path, and num_rows - specify how many rows we want to generate
load_config(file_path): loads the database connection configuration from a JSON file - file_path.
create_mysql_engine(config): creates an SQLAlchemy engine for MySQL connection and read credentials from json file.
get_data_from_file(path): loads data from a CSV file to a DataFrame.
put_data_into_mysql(pdf, con, table_name): loads data into a MySQL table using pandas method: to_sql.
Method DataFrame.to_sql takes two parameters (in my use case): con as sqlalchemy.engine and name which is table name.

File migrate.py
The migrate.py script migrates data from MySQL to PostgreSQL, transfering data from the mysql_sample_table table
to the pg_sample_table table in PostgreSQL.
Functions:
create_postgres_connection(config): creates a PostgreSQL connection.
create_staging_table(cursor): creates a temporary table in PostgreSQL for data migration. As a cursor it takes SQL DDL
statement.
read_data_from_mysql(query, con): reads data from MySQL. Method pandas.read_sql takes two parameters (in my use case):
a SQL query and con as sqlalchemy.engine - used in generate_data.py script, create_mysql_engine(config).
insert_data_to_postgres(connection, data, page_size): migrates data to PostgreSQL in batches.
This function takes three parameters: connection - which is provided by create_postgres_connection function,
data - which is provided by read_data_from_mysql function and the page_size.
page_size - maximum number of argslist items to include in every statement. If there are more items the function
will execute more than one statement. This is definition from https://www.psycopg.org/docs/extras.html
It determines how many rows are inserted per INSERT statement. For example, we want to migrate 10 000 rows,
this function will insert data in 10 batches (each with 1000 rows) executing 10 seperate INSERT statements. It matters
when we want move large datasets.
I found out about it from this interesting article https://hakibenita.com/fast-load-data-python-postgresql. It helped me
choose an efficient way to load data.
I have ran some tests by manipulating this parameter and method.
First method psycopg2.extras.execute_batch with page_size 10000:
Time: 135.10 s Memory: 1363.80 MB
Same method with smaller page_size 1000
Time: 108.41 s Memory: 209.61 MB - a bit quicker but a lot of less memory usage
Second method psycopg2.extras.execute_values page_size 20000
Time: 90.59 s Memory: 1131.79 MB
Same method with smaller page_size 2000
Time: 73.39 s Memory: 256.86 MB - it seems this is a satisfying outcome
In conclusion the results shows the more rows in one batch the more time it takes and consumes more memory.
So in my case, migrating 2 millions rows the best scenario is to choose execute_values method with page_size = 2000
Last function migration_memory_usage(connection, data): measures the time taken and memory usage for the migration
process.
File setup-environment.py
The setup-environment.py script automates the environment setup process. It installs all required dependencies,
starts the Docker containers, and runs the data generation and migration scripts.
Functions:
install_requirements(): installs dependencies from requirements.txt.
setup_docker_containers(): starts the Docker containers.
run_generate_data(): runs the data generation script.
run_migrate_data(): runs the data migration script.